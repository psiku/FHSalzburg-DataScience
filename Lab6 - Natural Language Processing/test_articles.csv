topic;link;header;article
artificial-intelligence;/article/game-of-thrones-season-8-predictions;Who will die in Game of Thrones season 8? Science has the answer;"Daenerys has a one per cent chance of death; Arya is on 47 per cent and Bronn, poor Bronn, has a 94 per cent chance of snuffing it Game of Thrones is back. Expect more dragon-duelling, sword-slashing, Cersei-scheming, dead-defeating, breast-bearing, willy-waving, incest-indulging fun as season eight finally hits our screens. When will we see Cleganebowl? Who will return as an icey zombie? And how will you fill the approaching void in your emotional lives? Drink? Political organising? Thankfully, there’s not long to wait.

While the answers to these questions remain unclear, one group of scientists over at the Technical University of Munich have had a crack at answering the most pressing one of all: namely, which Game of Thrones characters will die. All your favourites are ranked: Daenerys: one per cent chance of death; Arya: 47 per cent chance of death; Bronn, with the most likely demise of all the characters left on the show, a whopping 94 per cent chance of elimination.

The team reached such specific conclusions with the help of a deep neural network. “Our research group, the Rostlab, tries to answer questions such as, ‘How do mutations in our DNA lead to cancer?’” explains Guy Yachdav, team leader on the project. “We try to answer these questions using machine learning algorithms and data. In this project, we wanted to leverage our technical know-how and passion for data analysis to answer the Game of Thrones questions we were obsessed about.”

The team’s first Game of Thrones predictions came back in 2016, and they were broadly correct: the algorithm managed to predict that (spoiler alert) Jon Snow would be resurrected and that Tommen Baratheon, Peter Baelish and Stannis Baratheon would die. But even when the predictions turned out to be wildly wrong, they were still interesting. “So Daenerys is right now, according to aggregate, predicted a 0.9 per cent likelihood of death, and three years ago, we predicted her to be 95 per cent likely to die”, says Christian Dallago a scientific researcher and another member of the team. “But since that season, George R.R. Martin has lost control and other writers are writing the story. It's a little bit differently from originally intended, and that seems to have really had an effect on the data.”

Back in 2016, data sets were extracted from the books. But this year the data was taken from the two Game of Thrones wikis: A Wiki of Fire Ice and the Game of Thrones Wiki. The team created two predictive models. The first used the wikis to extract several data points on the shows 2,028 characters. These included age, gender, marital status, which books a character appeared in and whether their spouse was dead or alive. After analysing this data and extracting the most important characteristics, they fed it into the deep neural network, which spat out a predicted likelihood of death. Tyrion Lannister, for example, has a three per cent chance of death.

The second model is the character’s longevity. One of the team, Tatyana Goldberg, had worked on this type of analysis before, and suggested its applicability to Game of Thrones. Much like scientific studies might examine the correlations between seismic events, this technique assumes that for every year of a character’s life, there is some probability that they will die, and that certain variables will influence this outcome.

Taking Tyrion again, he is a Lannister, a male and a major character. Being a Lannister in Westeros is good for your longevity rate: it makes you statistically 44.9 per cent less likely to die. Yet being a male is bad: 22 per cent of men have been wiped out, compared to 11 per cent of women. Finally, being a major character ups Tyrion’s survival rate (16 per cent), but only slightly – this is Game of Thrones after all, where major characters drop like flies.

Since deep learning has many more serious applications, the team drew raised eyebrows from colleagues, says Dallago. But both he and Yachdav emphasise that the project was extremely helpful as a learning tool. “In our JavaScript seminar we try to find engaging ways to introduce our students into the world of data science, and working on problems that are tightly related to pop culture had done the trick,” says Yachdav. “Our students were as fascinated with their data science tasks as they are with the story of Game of Thrones.” Partly though, says Dallago, the project was just for fun. “We really didn't approach this the scientific way of going about it. Of course, don’t get me wrong, we wanted to have the best predictions.”"
artificial-intelligence;/article/artificial-intelligence-extended-intelligence;Forget about artificial intelligence, extended intelligence is the future;"We should challenge the cult of Singularity. AI won't take over the world Last year, I participated in a discussion of The Human Use of Human Beings, Norbert Weiner’s groundbreaking book on cybernetics theory. Out of that grew what I now consider a manifesto against the growing singularity movement, which posits that artificial intelligence, or AI, will supersede and eventually displace us humans.

The notion of singularity – which includes the idea that AI will supercede humans with its exponential growth, making everything we humans have done and will do insignificant – is a religion created mostly by people who have designed and successfully deployed computation to solve problems previously considered impossibly complex for machines.

They have found a perfect partner in digital computation, a seemingly knowable, controllable, machine-based system of thinking and creating that is rapidly increasing in its ability to harness and process complexity and, in the process, bestowing wealth and power on those who have mastered it.

In Silicon Valley, the combination of groupthink and the financial success of this cult of technology has created a feedback loop, lacking in self-regulation (although #techwontbuild, #metoo and #timesup are forcing some reflection). On an S-curve or a bell curve, the beginning of the slope looks a lot like an exponential curve. According to systems-dynamics people, however, an exponential curve shows a positive feedback curve without limits, self-reinforcing and dangerous.

In exponential curves, Singularitarians see super-intelligence and abundance. Most people outside the Singularity bubble believe that natural systems behave like S-curves, where systems respond and self-regulate. When a pandemic has run its course, for example, its spread slows and the world settles into a new equilibrium. The world may not be in the same state as before the pandemic or other runaway change, but the notion of singularity – especially as some sort of saviour or judgment day that will allow us to transcend the messy, mortal suffering of our human existence – is fundamentally a flawed one.

This sort of reductionist thinking isn’t new. When the psychologist BF Skinner discovered the principle of reinforcement and was able to describe it, we designed education around his theories.

Scientists who study learning now, however, know that behaviourist approaches like Skinner’s only work for a narrow range of learning – but many schools nonetheless continue to rely on drill and practice and other pillars of reinforcement. Take, as another example, the field of eugenics, which incorrectly over-simplified the role of genetics in society. This movement helped fuel the Nazi genocide by providing a reductionist scientific view that we could “fix humanity” by manually pushing natural selection. The echoes of that horror exist today, making taboo almost any research that would link genetics with, say, intelligence.

While one of the key drivers of science is to elegantly explain the complex and increase our ability to understand, we must also remember what Albert Einstein said: “Everything should be made as simple as possible, but no simpler.” We need to embrace the unknowability – the irreducibility – of the real world that artists, biologists and those who work in the messy world of liberal arts and humanities are familiar and comfortable with. Today, it is obvious that most of our problems – for instance, climate change, poverty, chronic disease or modern terrorism – are the result of our pursuit of the Singularity dream: exponential growth. They are extremely complex problems produced by tools used to solve past problems, such as endlessly pushing to increase productivity or to exert control over systems that have, in fact, become too complex to control.

In order to effectively respond to the significant scientific challenges of our times, I believe we must respect the many interconnected, complex, self-adaptive systems across scales and dimensions that cannot be fully known by or separated from observer and designer.

In other words, we are all participants in multiple evolutionary systems with different fitness landscapes at different scales, from our microbes to our individual identities to society and our species. Individuals themselves are systems composed of systems of systems, such as the cells in our bodies that behave more like system-level designers than we do. As Kevin Slavin says in his 2016 essay Design as Participation: “You’re not stuck in traffic, you are traffic.”

Biological evolution of individual species (genetic evolution) has been driven by reproduction and survival, instilling in us goals and yearnings to procreate and grow. That system continually evolves to regulate growth, increase diversity and complexity, and enhance its own resilience, adaptability and sustainability. We could call it “participant design” – design of systems as and by participants – that is more akin to the increase of a flourishing function, where flourishing is a measure of vigour and health rather than scale, money or power.

Machines with emergent intelligence, however, have discernibly different goals and methodologies. As we introduce such machines into complex adaptive systems such as the economy, the environment or health, I see them augmenting, not replacing, individual humans and, more importantly, augmenting such systems. Here is where the problematic formulation of “artificial intelligence” as defined by many Singularitarians becomes evident, as it suggests forms, goals and methods that stand outside of interaction with other complex adaptive systems.

Instead of thinking about machine intelligence in terms of humans vs machines, we should consider the system that integrates humans and machines – not artificial intelligence but extended intelligence. Instead of trying to control or design or even understand systems, it is more important to design systems that participate as responsible, aware and robust elements of even more complex systems.

We must question and adapt our own purpose and sensibilities as observers and designers within systems for a much more humble approach: humility over control.

Joi Ito is director or MIT’s Media Lab. This an edited excerpt of an essay originally published in the MIT’s Journal of Design and Science and which became a call for responses."
artificial-intelligence;/article/marcus-du-sautoy-book-extract-creativity-code;What's the purpose of humanity if machines can learn ingenuity?;"Flashes of inspiration are considered a human gift that drives innovation – but the monopoly is over. AI can be programmed to invent and refine ideas and connections. What's next? The value placed on creativity in modern times has led to a range of writers and thinkers trying to articulate what it is, how to stimulate it, and why it is important. It was while sitting on a committee at the Royal Society assessing what impact machine learning was likely to have on society in the coming decades that I first encountered the theories of Margaret Boden. Her ideas struck me as the most relevant when it came to addressing creativity in machines.

Boden is an original thinker who has managed to fuse many disciplines: philosopher, psychologist, physician, AI expert and cognitive scientist. In her eighties now, with white hair flying like sparks and an ever active brain, she is enjoying engaging enthusiastically with the prospect of what these “tin cans”, as she likes to call computers, might be capable of. To this end, she has identified three different types of human creativity.

Exploratory creativity involves taking what is there and exploring its outer edges, extending the limits of what is possible while remaining bound by the rules. Bach’s music is the ­culmination of a journey Baroque composers embarked on to explore tonality by weaving together different voices. His preludes and fugues push what is possible before breaking the genre open and entering the Classical era of Mozart and Beethoven. Renoir and Pissarro re-conceived how we could visualise the world around us, but it was Monet who really pushed the boundaries, painting his water lilies over and over until his flecks of colour dissolved into a new form of abstraction.

Mathematics revels in this type of creativity. The classification of finite simple groups is a tour de force of exploratory creativity. Starting from the simple definition of a group of symmetries – a structure defined by four simple axioms – mathematicians spent 150 years producing a list of every conceivable element of symmetry, culminating in the discovery of the Monster Symmetry Group, which has more symmetries than there are atoms in the Earth and yet fits into no pattern of other groups. This form of mathematical creativity involves pushing the limits while adhering to the rules of the game. It is like the explorer who thrusts into the unknown but is still bound by the limits of our planet.
Boden believes that exploration accounts for 97 per cent of human creativity. This is the sort of creativity that is perfect for a computational mechanism that can perform many more calculations than the human brain. But is it enough? When we think of truly original creative acts, we generally imagine something utterly unexpected. The second sort of creativity involves combination. Think of how an artist might take two completely different constructs and seek to combine them. Often the rules governing one world will suggest an interesting framework for the other. Combination is a powerful tool in the realm of ­mathematical creativity.

The eventual solution of the Poincaré Conjecture, which describes the possible shapes of our universe, was arrived at by applying very different tools to understand flow over surfaces. It was the creative genius of Grigori Perelman that realised the way a liquid flows over a surface could unexpectedly help to classify the surfaces that might exist.

My research takes tools from number theory to understand primes and applies them to classify possible symmetries. The symmetries of geometric objects at first sight don’t look like numbers. But applying the language that has helped us navigate the mysteries of the primes, and replacing primes by symmetrical objects, has revealed surprising insights into the theory of symmetry.

The arts have also benefited greatly from this form of cross-fertilisation. Philip Glass took ideas he learned from working with Ravi Shankar and used them to create the additive process that is at the heart of his minimalist music. Zaha Hadid combined her knowledge of architecture with her love of the pure forms of the Russian painter Kazimir Malevich to create a unique style of curvaceous buildings. In cooking, too, creative master chefs have fused cuisines from opposite ends of the globe.

There are interesting hints that this sort of creativity might also be perfect for the world of AI. Take an algorithm that plays the blues and combine it with the music of Boulez and you will end up with a strange hybrid ­composition that might just create a new sound world. Of course, it could also be a dismal cacophony. The coder needs to find two genres that can be fused algorithmically in an interesting way. It is Margaret Boden’s third form of creativity that is the more ­mysterious and elusive, and that is transformational creativity. This describes those rare moments that are complete gamechangers. Every art form has these gear shifts. Think of Picasso and Cubism, Schoenberg and atonality, Joyce and modernism. These moments are like phase changes, as when water suddenly goes from a liquid to a solid.

This was the image that Goethe hit on when he sought to describe wrestling for two years with how to write The Sorrows of Young Werther, only for a chance event to act as a sudden catalyst: “At that instant, the plan of Werther was found; the whole shot together from all directions, and became a solid mass, as the water in a vase, which is just at the freezing point, is changed by the slightest concussion into ice.”

Quite often these transformational moments hinge on changing the rules of the game, or dropping an assumption that previous generations had been working under. The square of a number is always positive. All molecules come in long lines not chains. Music must be written inside a harmonic scale structure. Faces have eyes on either side of the nose. At first glance it would seem hard to program such a decisive break, and yet there is a meta-rule for this type of creativity. You start by dropping constraints and see what emerges. The art, the creative act, is to choose what to drop or what fresh constraint to introduce such that you end up with a new thing of value.

If I were asked to identify a transformational moment in mathematics, the creation of the square root of minus one in the mid-16th century would be a good candidate. This was a number that many mathematicians believed did not exist. It was referred to as an imaginary number (a derogatory term Descartes came up with to indicate that of course there was no such thing). And yet its creation did not contradict previous mathematics. It turned out it had been our mistake to exclude it. How can a computer come up with the concept of the square root of minus one when the data it is fed will tell it that there is no number whose square can be negative? A truly creative act sometimes requires us to step outside the system and create a new reality. Can a complex algorithm do that?

The emergence of the Romantic movement in music is in many ways a catalogue of rule breaking. Instead of moving between close key signatures as Classical composers had done, upstarts like Schubert chose to shift key in ways that deliberately broke expectations. Schumann left chords unresolved that Haydn or Mozart would have felt the need to complete. Chopin in turn composed dense moments of chromatic runs and challenged rhythmic ­expectations with his unusual accented passages and bending of tempos. The move from one musical movement to another – from Medieval to Baroque to Classical to Romantic to Impressionist to Expressionist and beyond – is a story of breaking the rules. Each movement is dependent on the one before to appreciate its creativity. It almost goes without saying that historical context plays an important role in allowing us to define something as new. Creativity is not an absolute but a relative activity. We are creative within our culture and frame of reference. Can a computer initiate this kind of phase change and move us into a new musical or mathematical state? That seems a challenge. Algorithms learn how to act based on the data that they interact with. Won’t this mean that they will always be condemned to producing more of the same?

As Picasso once said: “The chief enemy of creativity is good sense.” That sounds on the face of it very much against the spirit of the machine. And yet you can program a system to behave ­irrationally. You can create a meta-rule that will instruct it to change course. As we shall see, this is in fact something machine learning is quite good at.
 Many artists like to fuel their own creation myth, appealing to external forces as responsible for their creativity. In Ancient Greece poets were said to be possessed by the muses, who breathed inspiration into the minds of men, sometimes sending them insane in the process. For Plato, “a poet is holy, and never able to compose until he has become inspired, and is beside himself and reason is no longer in him… for no art does he utter but by power divine”. Ramanujan, the great Indian mathematician, likewise attributed his great insights to ideas he received in his dreams from his family goddess Namagiri. Is creativity a form of madness or a gift of the divine?

One of my mathematical heroes, Carl Friedrich Gauss, was one of the worst at covering his creative tracks. Gauss is credited with creating modern number theory with the publication in 1798 of one of the great mathematical works of all time: Disquisitiones ­Arithmeticae. When people tried to read the book to uncover where he got his ideas, they were mystified. The work has been described as a book of seven seals. Gauss seems to pull ideas, like rabbits, out of a hat, without ever really giving us an inkling of how he achieved this magic. When challenged, he retorted that an architect does not leave the scaffolding after the house is complete. Gauss, like Ramanujan, attributed one revelation to “the grace of God”, saying he was “unable to name the nature of the thread which connected what I ­previously knew with that which made my success possible”. Yet the fact that an artist may be unable to articulate where their ideas came from does not mean that they followed no rules. Art is a conscious expression of the myriad of logical gates that make up our unconscious thought processes. There was of course a thread of logic that connected Gauss’s thoughts: it was just hard for him to articulate what he was up to – or perhaps he wanted to preserve the mystery, to fuel his image as a creative genius. Coleridge’s claim that the drug-induced vision of “Kubla Khan” came to him in its entirety belies all the preparatory material that shows the poet working on the ideas before that fateful day when he was interrupted by the person from Porlock. Of course, this makes for a good story. Even my own account of creation will focus on the flash of inspiration rather than the years of preparatory work I put in.

We have an awful habit of romanticising creative genius. The solitary artist is frankly a myth. In most instances what looks like a step change is actually a continuous growth. Brian Eno talks about the idea of “scenius”, not genius, to acknowledge the community from which creative intelligence often emerges. The American writer Joyce Carol Oates agrees: “Creative work, like scientific work, should be greeted as a communal effort – an attempt by an individual to give voice to many voices, an attempt to synthesize and explore and analyse.”

What does it take to stimulate creativity? Might it be possible to program it into a machine? Are there rules we can follow to become creative? Can creativity, in other words, be a learned skill? Some would say that to teach or program is to show people how to imitate what has gone before, and that imitation and rule-following are both incompatible with creativity. And yet we have examples of creative individuals all around us who have studied and learned and improved their skills. If we study what they do, could we imitate them and ultimately become creative ourselves?

These are questions I find myself asking every term. To receive their PhDs, doctoral candidates in mathematics have to create a new mathematical construct. They have to come up with something that has never been done before. My task is to teach them how to do that. Of course, at some level they have been training to do this already. Solving problems involves personal creativity even if the answer is already known. That training is an absolute pre–requis­ite for the jump into the unknown. By rehearsing how others have come to their breakthroughs you hope to provide the environment to foster your own creativity. And yet that jump is far from guaranteed. I can’t take anyone off the street and teach them to be a creative mathematician. Maybe with ten years of training, but not every brain seems able to achieve mathematical creativity. Some people appear able to achieve creativity in one field but not another, yet it is difficult to understand what makes one brain a chess champion and another a Nobel-winning novelist.

Margaret Boden recognises that creativity isn’t just about being Shakespeare or Einstein. She distinguishes between what she calls “psychological creativity” and “historical creativity”. Many of us achieve acts of personal creativity that may be novel to us but historically old news. These are what Boden calls moments of psychological creativity. It is by repeated acts of personal creativity that ultimately one hopes to produce something that is recognised by others as new and of value. While historical creativity is rare, it emerges from encouraging ­psychological creativity.

My recipe for eliciting creativity in students follows the three modes of creativity that Boden identified. ­Exploration is perhaps the most obvious path. First understand how we’ve come to the place we are now and then try to push the boundaries just a little bit further. This involves deep immersion in what we have created to date. Out of that deep understanding might emerge something never seen before. It is often important to impress on students that there isn’t very often some big bang that resounds with the act of creation. It is gradual. As Van Gogh wrote: “Great things are not done by impulse but by a series of small things brought together.”

Boden’s second strategy, combinational creativity, is a powerful weapon, I find, in stimulating new ideas. I often encourage students to attend seminars and read papers in subjects that don’t appear to connect with the problem they are tackling. A line of thought from a disparate bit of the ­mathematical universe might resonate with the problem at hand and stimulate a new idea. Some of the most creative bits of science are happening today at the junctions between the disciplines. The more we can come out of our silos and share our ideas and problems, the more creative we are likely to be. This is where a lot of the low-hanging fruit is to be found.

At first sight transformational creativity seems hard to harness as a strategy. But again the goal is to test the status quo by dropping some of the constraints that have been put in place. Try seeing what happens if we change one of the basic rules we have accepted as part of the fabric of our subject. These are dangerous moments because you can collapse the system, but this brings me to one of the most important ingredients needed to foster creativity – and that is embracing failure. Unless you are prepared to fail, you will not take the risks that will allow you to break out and create something new. This is why our education system and our business environment, both realms that abhor failure, are often terrible environments for fostering creativity. It is important to celebrate the failures as much as the successes in my students. Sure, the failures won’t make it into the PhD thesis, but we learn so much from failure. When I meet my students I repeat again and again Samuel Beckett’s call to “fail again, fail better”.

Are these strategies that can be written into code? In the past the top-down approach to coding meant there was little prospect of creativity in the output of the code. Coders were never too surprised by what their algorithms produced. There was no room for experimentation or failure. But this all changed recently: because an algorithm, built on code that learns from its failures, did something that was new, shocked its creators, and had incredible value. This algorithm won a game that many believed was beyond the abilities of a machine to master. It was a game that required creativity to play.

It was the news of this breakthrough that triggered my recent existential crisis as a mathematician.

This is an edited extract from Marcus Du Sautoy’s new book, The Creativity Code, published on March 7 (HarperCollins)"
artificial-intelligence;/article/avoid-facial-recognition-software;How to hack your face to dodge the rise of facial recognition tech;"Use of facial recognition tech is on the rise, but how do you get away from it? 3D-printed face masks, makeup, infrared lights, and complex patterns are being used to dodge its all-seeing eye UK police forces are increasingly experimenting with controversial new facial recognition (FR) technology for crowd control and locating suspects. Critics, however, have labeled the trials a shambles, pointing to the high error rate and even higher cost of the program.

Documents released under Freedom of Information Act requests have shown that collectively South Wales Police and London's Metropolitan Police have spent millions of pounds on trials of the technology, despite the fact that both systems have been shown to have an error rate over 90 per cent.

Similar trials around the world have raised concerns around the technology, including in San Francisco where privacy advocates are calling for a ban on the use of FR by law enforcement.

It’s not just the police who are interested in the potential use of FR. From shopping malls to sporting grounds, it is becoming more and more difficult for the average person to know when this technology is being used to track them, by whom and for what purposes. And while FR may be error-prone now, this is unlikely to stay the case for long. How comfortable the public is with the use of FR in public spaces is likely to vary depending on the context. Many people may be fine with police using FR for crowd control at major events, for example, but not with the same technology being used to track them around the supermarket aisles in an attempt to up-sell them on potatoes.

The role FR will play in societies will be decided after a broad and complex debate that's likely to take many years. The answer will almost certainly have to include some form of regulation to control how such a powerful technology is used.

But until the sticky wheels of regulation grind into gear, what are the options for people who want to walk around in public without being constantly identified?

“Depending on the kind of technology that's being used, you can attempt to deflect FR in many different ways,"" says pen tester and privacy advocate Lilly Ryan. “You really need to know what’s under the hood to know what is most likely to work, and it can be very hard for the average person to know what kind of FR is being used on them at any particular time.”

Most research on FR systems is conducted under lab conditions, in which researchers know exactly what kind of FR system they’re working with and often also have access to the underlying code and even the training data, giving them a huge head-start in fooling the system which they would be unlikely to have ‘in the wild’. In the real world, FR is also often combined with other biometrics such as fingerprints or gait analysis. The introduction of increasingly powerful AI techniques has also provided a huge boost to the field. ""The progress achieved in FR after the integration of deep learning is exponential,"" says Christoph Busch of the Norwegian University of Science and Technology. Busch and his colleague Raghavendra Ramachandra have studied FR systems extensively, including surveying known ways to fool the technology.

Faced with sufficiently sophisticated systems, the reality is that there are no truly guaranteed methods of avoiding identification. However, many FR systems actually in use today are not all that sophisticated, and researchers and privacy advocates are finding ways to beat the technology.

Occlusion and confusion Techniques for fooling FR can be roughly divided into two categories: occlusion or confusion.

Occlusion techniques work by physically hiding facial features so the camera simply can’t see them. How successful these methods are will depend on which bits of your face are hidden and how well hidden they are. For example, a balaclava which leaves the most important facial features exposed – the eyes, the mouth, the nose – may not actually do much to prevent a person from being identified. Researchers have found that by using a deep learning framework trained on 14 key facial points, they were able to accurately identify partially-occluded faces most of the time. This includes wearing glasses, scarves, hats or fake beards,

If you really want to take it to the extreme you can skip the ski mask and go straight to a 3D-printed model of someone else's face. The deeply unnerving URME Personal Surveillance Identity Prosthetic is a 3D scan of the artist Leo Selvaggio's face, right down to his hair and skin texture.

If, on the other hand, a more or less literal tinfoil hat is your thing then Project Kovr, a jacket which zips up over your head and looks like it could double as a spacecraft may be the option for you.

However, while hiding your entire face may be more effective for preventing FR systems from identifying you, it’s the exact opposite of inconspicuous. Completely obscuring your face is illegal in many places, including many places in Europe, Canada and the United States.

Even where it may be technically allowed, it’s hard to think of a quicker way of attracting attention from the people around you – not to mention the police – than walking down the street looking like you’ve just escaped from a sci-fi dystopia. Confounding the computer So if occlusion is uncertain at best and liable to get you locked up at worst, that leaves confusion. One of the most straightforward techniques is to stop the FR system working is to make it think it isn't looking at a face.

“If you’re attacking the facial detection stage, you could try and break up the lines of your face to try and stop it from being detected by the system in the first places,” Ryan says.

This is the idea behind CV Dazzle, which uses extreme makeup and hairstyles to bewilder computer vision systems. This technique (and other forms of extreme makeup, like Juggalo makeup) confuse computer vision systems by playing with light and darkness in a way which makes a face look – to a computer – like it’s not a face.

""From an academic research perspective, the 'makeup attack' is gaining more attention. However, this kind of attack demands good make up skills in order to be successful,"" Busch observes. Just applying makeup at random is unlikely to be enough – key facial points need to be obscured in specific ways in order to fool the system. Dazzle methods will only work on systems which rely on visible light, however (and like some of the occlusion methods, it is likely to get you a lot of attention when you're walking around in public). That means it’s not applicable to more sophisticated systems like Apple’s FaceID, which use infrared light rather than visible light.

“They bounce the beams of [infrared] light back off your face to create a 3D map of your face. That can make it even harder for people to avoid detection because it’s not just relying on the way that you appear, but also the contours of your face,” Ryan says. “Which is really useful when it comes to say, the iPhone not being fooled by a flat printed image because you can’t get any contours off it.”

Infrared-based systems may see through techniques like CV Dazzle, but they are vulnerable to other forms of interference. It’s possible to fight infrared with infrared.

In 2018, researchers from Fudan University in China, the Chinese University of Hong Kong, Indiana University, and Alibaba Inc., used an array of tiny infrared LEDs wired to the inside of a baseball cap to project dots of light onto the wearer’s face. These dots were invisible to the human eye, but confused the computer vision and made the face unidentifiable.

The researchers found that they couldn’t just hide the wearer’s identity, however – they could also make the computer think they were someone else altogether. Using the LEDs, in 70 per cent of tests the researchers were able to trick the FR system into thinking their colleague was Moby. Exploiting expectations
In order to see this embed, you must give consent to Social Media cookies. Open my cookie preferences.

If makeup and LEDs are about disguise, HyperFace camouflage is about distraction. Instead of trying to stop the system from detecting a face at all, the goal is to overwhelm it by making it see way, way too many faces. The pattern can be printed onto scarves or earrings, or anything which can be worn close to a person's real face.

FR systems detect faces based on specific patterns of light and darkness. What the HyperFace camouflage does is mimic those patterns of light and dark in a way which looks like a face to computer vision, but not to human eyes. The goal of HyperFace is to make your real face a needle in a haystack for FR systems, whilst being relatively inconspicuous (beyond just wearing a cool scarf) to the people around you.

“In other words, if a computer vision algorithm is expecting a face, exploit its expectations,” write the creators of HyperFace. At this stage, however, the project is still only a prototype.

Built-in bias
Intentionally trying to fool the technology is one thing, but being the subject of an unintended error is quite another. Many FR systems tested in the real world have proven to be stunningly inaccurate. 92 per cent of matches made by the system trialled by police during the UEFA Champions League Final week in Wales in 2017 turned out to be wrong, for example. Not only are FR systems often inaccurate, but they can be biased along racial and gender lines. Researchers such as MIT Media Lab’s Joy Buolamwini have called out companies for failing to train their technology to see diverse faces.

“Unfortunately it turns out that a lot of the ways to confuse FR are to be in a demographic who are not highly represented in the data which these systems are trained on, which quite often ends up being people who are not light-skinned, or not male presenting,” says Ryan.

“So part of it is that, sure, trying to trick this technology is fun, but there are also very serious consequences for being mistaken for somebody else, particularly in a law enforcement context.”

But things will change in the long term. The high levels of inaccuracy in FR trials are unlikely to last long. “Emerging deep learning techniques together with the availability of the large-scale face images through social media, and the progress in computational resources through GPUs has significantly improved the recognition performance of face recognition system,” Busch and Ramachandra say.

The technology is developing particularly rapidly thanks to enormous resources being thrown at it by both governments and private corporations. They’re not spending that money for the sake of scientific endeavour – they’re investing in the technology because they plan to use it. As the use of FR becomes more prevalent in the places we move through in our daily lives, the concerns associated with it become sharper. Any method for evading recognition is at best a stopgap solution, particularly when FR is combined with other forms of biometric identification.

The real solution to the issues around FR? The tech community working with other industries and sectors to strike an appropriate balance between security and privacy in public spaces."
artificial-intelligence;/article/deepmind-ai-chess;DeepMind’s superhuman AI is rewriting how we play chess;"AlphaZero doesn’t play chess like a machine – it plays it like a human grandmaster, but better Since 1997, when IBM’s Deep Blue beat world champion and chess legend Garry Kasparov in a six-game match, chess players have accepted that machines are stronger at chess. We have taken some comfort from the fact that we taught these machines how to play. But strangely enough, despite being programmed by humans, traditional chess engines don’t play quite like humans. AlphaZero’s reinforcement learning has given it a distinctive and instantly recognisable style, and it implements its ideas in a direct, efficient way, without undue regard for the material balance. It has human-like drive to make progress and never sit still. Interestingly, many of AlphaZero’s ideas match accepted human rules derived from hundreds of years of playing chess. However, AlphaZero’s twist (achieved through its deep neural network architecture) is to combine factors we considered minor or incidental – such as the restriction of the opponent’s king – into a whole game strategy. For example, taking unusually early action to create a weakness in the opponent’s king’s position and then using this weakness as a motif throughout the rest of its play.

Having AlphaZero next to us felt like having a human chess genius on tap, who never got tired and never asked for coffee. “AlphaZero find us a path!” became our standard cry during the World Championship and it was always ready with a creative way to optimise its position. Its strength compared with traditional engines wasn’t necessarily in calculation-heavy positions but rather in intricate positions in which a mixture of calculation, positional insight and long-term planning was required. We particularly noticed how alert AlphaZero was to the danger of landing in a passive position without prospects and how driven it was to avoid this scenario.

In our book Game Changer: AlphaZero’s Groundbreaking Chess Strategies and the Promise of AI we work with the DeepMind technical team to explain how AlphaZero’s construction and training has led to its creative and intuitive style. There are many unexpected aspects to this. For example, AlphaZero trains by playing vast numbers of lightning-fast games (40 milliseconds a move) against itself at a very shallow search depth. There is a trade-off here: one might think that AlphaZero could learn more by playing slower, high-quality games. However the quicker the game is played, the more games AlphaZero sees, the more different situations it is exposed to, and the more it can learn. Quicker games are also more likely to become unbalanced and produce a decisive result which AlphaZero can then use to tune (strengthen or weaken) the connections in its policy network that led to its decisions in the game. There is an interesting parallel in the way modern chess grandmasters train compared to 40 years ago. 40 years ago, “blitz chess” – super-fast games played with just one or three minutes per player per game – were frowned upon as both a waste of time and damaging for your chess skill. However, all of the current top chess players – World Champion Magnus Carlsen above all – are superlative blitz players and regularly take part in online blitz competitions.

Another fascinating aspect is how AlphaZero evaluates chess positions. Traditional engines evaluate a given position via a scale based on material (the general chess term for pawns and pieces). For example, a score of +1.5 indicates an advantage of one-and-a-half pawns. (The generally recognised scale for material in chess is that pawns are worth one point, knights and bishops are worth three points, a rook is worth five points and a queen is worth nine points.)

AlphaZero evaluates positions probabilistically based on its perceived chance of winning or drawing (in fact we don’t even know whether it assigns any values for pawns and pieces!) This may explain why AlphaZero is not afraid to sacrifice its pawns and pieces to achieve its goals: what does a pawn or two matter if your expected score increases?

The evaluation of traditional engines also reflects only the single best variation it finds in the position. AlphaZero’s evaluation is a weighted average of all the variations it considers in the position, not just the single best variation. This seems to allow AlphaZero to steer games “intuitively” into promising-looking situations, in which danger and the possibility of mistakes are ever-present for the opponent, without needing to calculate every detail – just like strong human players do.

AlphaZero’s strength and originality truly surprised us. Chess is full of superhuman expert systems, yet AlphaZero discovered an uncharted space in which its self-taught insights were both startling and valuable. That uncharted space was so significant that AlphaZero was able to convincingly defeat the strongest expert system at the time of testing. Bearing that in mind, you can’t help but to be positive for the application of AlphaZero-like techniques in environments that are less well-researched than chess. Maybe soon, scientists will be echoing our cry during the World Championship: “AlphaZero, find us a path!”

Despite the hand-crafted heuristics, the fundament of an engine’s superiority lies in calculation: sifting through vast numbers of moves to find concrete ways to solve a position. Back then, chess grandmasters were hired in to evaluate a series of typical positions and describe the considerations that led to the assessment, and then programmers turned these considerations into ever more sophisticated heuristics. A chess program or an “engine” like Stockfish searches through about 60 million positions a second. But an engine’s solution may look ugly to human eyes, even if it is unquestionably a winning move.

Enter DeepMind. The Google-owned AI company’s AlphaZero is a paradox. AlphaZero taught itself chess (as well as go and shogi) starting with no knowledge about the game beyond the basic rules. It developed its chess strategies by playing millions of games against itself and discovering promising avenues of exploration from the games it won and lost. It also searches far fewer positions than Stockfish when it plays. The result was a chess player of superhuman strength with a style that is human-like.

We worked together intensively with AlphaZero during the World Chess Championships played in London in November 2018. While Norway’s Magnus Carlsen and America’s Fabiano Caruana were fighting it out across the chessboard, AlphaZero was evaluating their moves and suggesting alternative ideas.  "
artificial-intelligence;/article/cedric-villani-france-artificial-intelligence;Meet the brain Macron tasked with turning France into an AI leader;"Cedric Villani is on a mission: making France an AI leader. Can he do it? In his office in Paris’s National Assembly, Cédric Villani opens a parcel: it contains a metallic spider. “Lovely,” he says, putting it on a shelf, where a collection of spider-shaped objects sits next to his scientific decorations and a photo of him with Mark Zuckerberg.

Villani is on a mission. Well, on several missions: the French mathematician, winner of the 2010 Fields Medal – often described as maths’ Nobel Prize – sits as an MP for Emmanuel Macron’s party La République en Marche, teaches at the University of Lyon, and is running for the Paris 2020 mayoralty. But the expert in mathematical analysis, famous for his academic achievements as well as for wearing spider-shaped pins on his three-piece suits, has a bigger goal: making France a leader in artificial intelligence.

Appointed by the French president to set out a national AI strategy, in 2018 Villani published a report, “AI for Humanity”, setting clear lines for the sector: “We must valorise our research, define our industrial priorities, work on the ethical and legal framework and on AI training,” Villani says, sat among his spiders – one as big a pillow – in his office.

Following the report’s publication, President Macron announced an investment of €1.5bn, over four years, to implement most of Villani’s propositions. “I want France to be one of the leaders of the AI sector,” Macron said. “We have the means, and we will create the conditions.” Villani identified four sectors to prioritise: health, defence, transport, and the environment. From early detection of pathologies to green urban mobility, AI can be used for “common good”, Villani concluded in the report, and recommended the creation of open data platforms for each sector. Data on energy, transport, agriculture, biodiversity, climate, agriculture and much more could help all four sectors, for example by forecasting traffic peaks, pollution, floods, and allowing a better organisation of French public life.

About 95 per cent of his propositions are being implemented by the government, Villani says; health is the sector in which the creation of the data platform is most advanced. But Villani estimates that the number of AI students must be tripled to answer the sector’s needs. The government is aiming to double it, and wants at least 40 per cent of researchers to be female. “We’re currently very far from these goals. It’s going to take very voluntarist actions,” Villani says.

Research is expected to play a pivotal role in Villani’s plan. On November 6, 2018, the minister for Research and Innovation, Frédérique Vidal, announced the creation of four “interdisciplinary AI institutes”, or “3IA”, in Paris, Toulouse, Grenoble and Nice, where academics and industrials will work together on AI projects around local priorities in health, education, robotics, the environment and public policy.

For the 3IA to be a success, France will need to retain talent. On Villani’s advice, the government is now trying to stem the scientific “brain drain”: to retain world-class French mathematicians, algorithmics experts and statisticians, who often leave low-paying roles in France for the US or China, a new law will allow them to take on better-paid consultancy work, for a maximum of 50 per cent of their time. Villani also suggested doubling researchers’ salaries, but the government turned that recommendation down. To Nozha Boujemaa, the research director at the National Institute for Research in Computer Science and Automation, which will coordinate the 3IA project, this is a problem: “We won’t be competitive enough,” she says.

The €1.5bn budget allocated by Macron shows France’s limits on competitiveness, too. “One billion is less than a regional budget in the US or China,” Boujemaa says. Villani knows France can’t compete with Chinese investment – the city of Tianjin alone has a £11.6bn AI budget – but it’s not about France, he says: it’s about Europe. “France will do nothing in the AI sector without Europe. We need networks of researchers and institutes throughout Europe, to work with each other’s strengths and good practices.” For this reason, Villani, finds Brexit “tragic”. “At a moment when we need all European skills, in a context of harsh competition worldwide, it is regrettable to lose the UK’s remarkable expertise. It is in France and the UK’s best interest to keep close links in the AI sector.”

France may not become the AI leader, but it can become an AI leader. Villani’s biggest challenge isn’t budget or international competition: it’s changing the cultural mindset. He gives the example of two French ministry services where an algorithm was tested to improve the office’s operations. Although its efficacy was proved, one test was stopped quickly, and Villani expects the other to stop, too, because people were wary of the changes introduced by AI. “It’s about cultural adaptation, about the whole engine being in the same mindset,” he says. “We must convince everyone to cooperate.”"
neuroscience;/article/consciousness-hacking-silicon-valley-enlightenment-brain;Inside Silicon Valley's new non-religion: consciousness hacking;"""I saw spiritual attainment and I thought, 'That does not need to be religious. That can be scientific.'"" It's 7:30pm on a chilly April night in San Francisco. The Battery, a members-only club in North Beach that serves Silicon Valley's ruling elite, hums with grey-bearded founders, pink-cheeked CEOs, serial startup kids and their venture-capitalist quarry. A herd of young men in fleece indulge in $1,400 (£1,080) whisky, then make their way to the roof deck, watched over by the bar's nautical décor: paintings of 19th-century steam ships and carved wooden figureheads. Though the club bans smartphone use after 6pm and forbids voice calls to ""protect the relaxed atmosphere"", a few stork-legged women float through, clutching their iPhones like little prayer books, texting and not-looking walking. The overall vibe is buzzy and self-affirming.

One flight up, in the club's library, two men pace a small stage. They cradle microphones TEDx-style, enlightening 200 or so people perched on velour ballroom chairs, tumblers in hand. ""Most of us are living in a highly distracted, over-stressed, ego-driven experience,"" says Jamie Wheal. Angular, with landscaped eyebrows and a methodic vocal cadence, Wheal lays out the central burden of our time: ""No one built an off switch,"" he says. To self-soothe, ""We rock Ambien on a nightly basis."" We binge-watch Netflix, drink three whiskies a night and ""jack off"" to YouPorn 24/7. We swipe Grindr, join Headspace, and Fitbit away our anxiety in a desperate bid to keep up. ""Everyone,"" he says sympathetically, ""is trying to alter their consciousness."" The crowd is equal parts loose and rapt, some there to enjoy the show, others to listen for a new direction forward. ""So one of the things to realise,"" says Wheal, as a chart titled ""Altered States Economy"" projects behind him, with the figure $4 trillion (£3.1tn) at its centre, ""is A, massive market. B, largely unintentional, this is hiding in plain sight.""

In other words, getting out of our own heads, tweaking the diodes on our emotions and consciousness, is a ""Four-trillion-dollar opportunity for the entrepreneurial-minded"". It is testament to an urgent need, but a fraught one. ""How do we make this positive and not destructive, distracting and addictive?"" he adds. ""Can we help steer people toward their better angels and our collective nature in our search for these things?"" Silicon Valley has always sought to mix engineering with enlightenment. After it hacked our desktops, our phones and then our attention spans, it sought to hack our corporeal selves. First came peak performance (smarter, faster, stronger) then mindfulness (chillax, brah). Today, the new frontier is consciousness hacking. Its goals are varied, its practitioners virtuously divided and its definitions fluid.

Its first wave came tumbling at us as biofeedback, meditation apps and micro-dosing. It crashed in the surf of sensory deprivation and DIY transcranial direct-current stimulation - sending small voltages across the skull with electrodes and a controller - to boost focus and reduce depression. It was driven, and still is, by a decade-old revolution in neurobiology and brain imaging that lets us look under the hood at mental states, figure out where they come from and how to get more of them using new tech.

That led to our next wave: pulling the neural triggers that can produce the same kind of enlightenment that lifelong meditators experience. Want an out-of-body experience? We have virtual-reality simulations for that. Want to be smarter and happier? You can learn to quiet your pre-frontal cortex - that inner critic - and access more of your brain's attention-focusing norepinephrine.

Wheal's gambit is this last bit. Along with Steven Kotler - the thin man with the grey Caesar cut next to him on stage - he is the co-founder of the Flow Genome Project. With it, they map flow states, using open-source input from athletes, artists, academics and others. And they will tell you how to achieve it via training videos, newsletters and interactive apps - all for an initiation fee of $697, then $97 per year.

Both men are at The Battery tonight to promote and sell copies their new book, Stealing Fire, one of a dozen they've written (individually) on peak performance. But unlike the others, this one is a user manual for hacking everyday nirvana. Much of it is based on the ancient Greek notion, and reports, of ecstasis (yes, ecstasy). That literally means to stand outside yourself, usually in a trance experience with God. Their book is packed with neuroscience and imaging research that definitively locates ecstatic states in the human brain, along with nods to data on how to get there. None of it is more unusual - or more understated - than the three-page section on a former MIT roboticist who popularised the term ""enlightenment engineering"". And though he has no app or video to sell, he turns out to be at the centre of evangelising for consciousness hacking. The morning after the Battery talk, I wake up in a $10 million mansion in the green, undulating canyons of San Carlos, 40 minutes south of the city. Outside the front door, at the bottom of a steep drive lined with cars, including a graffitied van, someone has parked a Tesla Roadster. Inside, mandalas and flip-flops cover the wood floors. Tacks hold Indian tapestries to the living-room walls. Junked hard-drive towers and monitors narrow the hallways. And the open-plan kitchen, as big as a yacht, bears signs directing you to the natural tea shelf and the composting bin.

Mikey Siegel pads into the kitchen wearing loose black jeans and rubber flip-flops. His angular head, which teeters atop his thin 178cm frame, is crowned by unruly black hair. A 35-year-old former MIT roboticist, Siegel founded the Consciousness Hacking MeetUp group in San Francisco in 2013. It has since spread worldwide and now claims 15,000 members in 30 communities. He also helped found the Transformative Technology Conference. It's a mixer for technologists, futurists, entrepreneurs and venture capitalists trying to turn consciousness hacking into a multi-billion-dollar business on a par with the fitness industry.

Siegel is a sweet, nerdy guy. When he speaks, he sounds like a cross between a precocious college undergrad who just awoke to pot and Plato and a genius engineer with a gift for research recall. He and six others rent this co-hack house from a venture-capitalist friend who has decamped to another home closer to the Bay Area. ""This is not woo-woo stuff,"" says Siegel, leading me out to a stone patio lined with lilac and overlooking an impossibly green canyon. Like many people proselytising consciousness hacking, Siegel knows how squishy it sounds to those outside the valley or beyond the research labs. He's quick to tick off a list of key Harvard and Stanford brain-scanning studies done on meditating Buddhist monks, showing increased neuroplasticity (a capacity to change) and deep, active neural correlates. ""These are 30,000-hour meditators,"" says Siegel. ""Their brains are profoundly different. Their experience of reality is profoundly different. They're not just a little bit happier."" Taking a seat and drinking a smoothie, he says, breathlessly, ""What does that mean if you can create the technology that makes that accessible to everyone? That's like, I don't know. It could alter all of humanity.""

""Most of us are living in a highly distracted, over-stressed, ego-driven experience. No one built an off switch""
Jamie Wheal, co-founder of the Flow Genome Project
Siegel is the high priest of the movement. ""I think I drink the Kool-Aid a little too much,"" he tells me at one point, to which a friend replies, ""You become the Kool-Aid."" His MeetUp group, which has 4,000 members, tackles topics such as quantifying bliss, dissolving the boundaries between self and other, and hack dating: connecting with confidence. On Project Nights, startup founders pitch their prototypes. One night it included the LucidCatcher, a sleep band that stimulates your brain with small electrical pulses, supposedly letting you control your dreams. So you could, say, be riding a dragon, talking to a long-dead parent or having sex with a famous celebrity.

Though Siegel is hosting a sold-out MeetUp for 120 people tonight - $10 per ticket or $100 for an annual membership - he's as relaxed as a California surfer. ""A lot of logistics get taken care of by other people these days,"" he says.

Siegel grew up in ""a nice Jewish family"" in Southern California. He earned a degree in computer engineering, then landed at Nasa's Ames Research Center, working on robotics. In 2006, he moved east to the MIT Media Lab and studied with social roboticist Cynthia Breazeal. While there, he worked under a research grant for Audi, focusing on persuasive robotics - figuring out how to give a car a social interface that would help it influence driver behaviour,"" he says, ""in a way that felt good."" Then, in 2011, he moved to a now infamous startup called Theranos. Siegel, who is verbose on most topics, is vague about what he did there. But it's possible that in such a new company, where titles and responsibilities overlap, it really is hard for him to pin down. ""I don't want to say I was like a rogue,"" he says, ""but I did everything. I did a lot of prototyping, designing, building and testing things,"" and reporting directly to its now embattled founder and CEO, Elizabeth Holmes. In typical venture-backed fashion, he also earned a ridiculous pay cheque.

He shared a house in San Francisco with friends, threw huge parties and hosted weekend-long concerts. ""I was living the dream,"" he says. ""And I felt like shit. I felt empty. I had the most privileged wake-up call, where you get all the things you think are going to make you happy, and they don't make you happy.""

Prior to his time at Theranos, he had stayed on a Virginia ashram and had trekked to India to meditate. ""It was like 'poof',"" he says, blinking hard and then looking at me deeply. ""I saw the pinnacle of spiritual attainment and I thought, 'That does not need to be religious. That can be scientific.' And I wanted to create the technology to get there."" With about $60,000 saved - and the safety net of a trust fund should he fail - Siegel set out. He wrote to eight potential academic advisers. ""Nobody had any fucking idea what I was talking about,"" he says. Then he found Gino Yu, an associate professor and director of digital entertainment and game development at Hong Kong Polytechnic University. Yu investigates what philosophers and scientists have long called the consciousness ""problem"" - the murky relationship between mind and body. He uses interactive media, like video games and VR, and new tools such as electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) to trace realms of perception, experience and the mind's inner workings in real time. Within an hour of talking to Yu, Siegel had an invitation to visit. A week later he was on a plane. Once there, Siegel met a group of acolytes who had gathered around Yu. Among them was Jeffery Martin, a Harvard psychology graduate who had bounced around jobs and had reinvented himself in this new space. With Martin's help, Siegel gorged himself on neuroscience literature and learned about emerging neurotechnology tools. ""I spent a full year catching up, getting the complete download,"" says Siegel.

Brain full, Siegel and Martin filled a whiteboard with ideas for consumer products. At the time, brain hackers had started using transcranial direct current stimulation (tCDCS), attaching nine-volt batteries to electrodes on their foreheads to mildly shock their brains to increase focus and improve memory. Siegel and Martin thought they could induce meditative states this way. When Martin strapped it on, he says, ""I had the most mystical experience of my life."" Their plan was to produce a device and create crowdsourced protocols - voltage, placement of electrodes, time of day - that would trigger enlightenment.

That plan, and others, fell apart. But one stuck. First, they came up with a name for their movement: consciousness hacking. ""Because it was avant-garde and eye-catching,"" Martin tells me. The pair envisioned a three-prong approach to foster their vision. It included an academic lab where they could study enlightenment and incubate the tech around it; a conference to connect hackers, biofeedback enthusiasts, and venture capitalists; and building out a larger cultural community.

""I saw spiritual attainment and I thought, 'That does not need to be religious. That can be scientific""
Mikey Siegel, founder of Consciousness Hacking MeetUp
After returning to the US, they launched their first Consciousness Hacking MeetUp, in 2013, in Siegel's hometown of Santa Cruz. A year later they held their first conference, at Sofia University in Palo Alto, under the broad and inclusive term Transformative Technology. Soon after, Siegel quit the conference arm, splitting the brands in two, with Martin and a third partner taking over the more industry-facing conference side and Siegel taking charge of the globally expanding MeetUp scene. A lot of what I do is just being a schmoozer,"" says Siegel. ""I'm passionate. A lot of it is connecting people with millions of other people, fertilising the space, creating a community, hosting events, speaking, travelling, that kind of stuff. Dealing with tonnes of emails.""

We're driving down a canyon in Siegel's black Ford Expedition, which is littered with iPhone cables and snack wrappers. In addition to organising MeetUps, he advises wearable-tech and bio-sensing startups on their market strategies, tweaking their algorithms and connecting them with software engineers and investors. One company, Sensi, uses an iPhone's accelerometer and artificial intelligence to analyse movement, determine your mood and guide you to emotional optimisation.

""A lot of people are sitting at their desks year after year after year and saying, 'What, more?'"" says Josh Whiton, who is outdoor-yoga lithe, tanned and incredibly erudite. ""Their garage is full of stuff and they don't want any more for Christmas. So this is a resurgence for extraordinary experiences and consciousness and mind expansion.""

Whiton is highly aware of his privilege and careful to point it out. In 2004, while a student at North Carolina State University in Durham, he began building TransLoc, a transit-tech company that tracks buses in real time. It's the official app for 300 municipal, university and corporate agencies. But feeling spiritually bereft by the work of a CEO, Whiton left several years ago to start an urban farm in Raleigh, take writing courses, trade cryptocurrencies, and, as stated on his LinkedIn page, become a consciousness explorer.

When we first met at the co-hack mansion, he and Siegel hugged warmly, with Whiton letting out a long ""Mmmmm"". Siegel said to me, ""This guy is the secret sauce."" He's also a reliable contributor to the co-hack house's rent and he invests in startups. ""Back in the 60s,"" Whiton goes on, as Siegel drives toward the blue of the bay, ""you had a lot of broke hippies living in their parents' basements with this general 'Fight the man, I don't want to sit at a desk and wear a tie rebellion.' To me, this is the 60s again, but with a new credibility."" Today, instead of broke hippies, you get people like Whiton and Siegel who are entrepreneurs and have had success. ""So here I am with time and privilege and credibility,"" says Whiton, who lunches with Elon Musk, counts Tony Robbins as a friend and still couch surfs. ""I can go around and ask the same questions hippies do. But when I have my adventures, I'm meeting not just hippies and shamans, but venture capitalists, engineers and stock traders. That's special.""

When I express doubt that technology, and technologists, who have created our over-stressed, socially anxious, always digitally-on selves can now create the off-button solution - through the same sort of tech - Siegel says, ""That's legitimate.""

""Tech can support our humanity and our true nature,"" says Whiton, who, I realise, is not spending this ride tapping at a phone, but is fully engaged. ""It just can't be tech products in the service of distracting you from your emotions, distracting you from your problems, which has been a major thrust of the economy. Sure, you're mildly depressed. But we've got a pill for you. Sure, you don't like your home life, but we've got happy hour for you. So much of the economy works on repression and distraction. But we're seeing a shift. We have the engineering knowledge to create the tools to get us back in touch with our humanity and to regulate our emotions.""

That consumers are desperate to relax and de-stress is clear. We're buying tens of millions of pounds' worth of biofeedback headsets and meditation apps to get there. The next engineering challenge is teaching tech to detect things about the human experience that matter to humans. People such as Martin believe that as artificial intelligence improves, we'll return each day to a smart home that, using facial recognition and other sensors, will know how we feel as soon as we walk through the door. Our home will provide us with the sounds, lights, smells, food and words that a loved one might provide. We'll carry biomarkers that detect our real-time cortisol levels (indicating stress) and headsets that guide us to a calmer state. Tech giants such as Google, Apple and Adobe, concerned about the work-life balance of their workers, already provide mindfulness training and talks for their employees. Some even distribute biofeedback headsets like the Muse. And Martin's Trans Tech conference is promoting consumer gadgets that are scalable (meaning they can be found on the walls of Best Buy) and could one day be reimbursable by insurance companies.

""Flow and empathy are about to have their mainstream moment,"" Siegel says, as he pulls into a flower- and vine-covered shopping centre where we'll eat breakfast at a place called Café Bliss. ""How many other altered states can we achieve? There are a hundred more out there. Are people ready for that? People are ready.""

And not just for ego-driven self improvement, Whiton argues, but for communal coherence. Siegel himself has designed a system called HeartSync, which he first deployed at Burning Man a few years ago. It links 24 people via EEG headsets to a computer hub. Using audio cues over speakers, it lets the group sync its collective heartbeat and breathing. ""So say you decided to have a board meeting and you hook everyone up to a headband and a heart-rate sensor to get into state of coherence,"" says Whiton. ""You get people out of their ego states and into a group state, thinking as one. And what kind of meeting would we have after that?"" By early evening Siegel, dressed in loose-fitting guru pants, is sitting at the head of a folding table, flanked by plastic-wrapped tubs of cheeses and cold cuts. Volunteers peel the plastic and set the goodies on nearby café tables. Each month, Siegel holds his MeetUps here at Eco-Systm, a co-working space just outside the city's financial district. The place has an artsy startup vibe. Tonight's talk features Julia Moss-bridge, a Northwestern University neuroscientist. She studies how our conscious and unconscious minds perceive and process time. Her talk, ""Designing an AI to Love"", will explore whether we can create an artificial intelligence that loves, unconditionally, with the welfare of humanity at its heart.

The MeetUp is sold out. As people drift in and nibble on the meats and chocolate-covered strawberries, Siegel drifts among them, exchanging lingering hugs and introducing people he knows to other people he knows. Among them, a married couple designing therapeutic VR experiences; the head of a Burning Man camp from Ohio, in town for the group's global leadership conference; and a smattering of young tech workers, including two women who develop user experiences for a healthcare company and had recently seen an AI talk at South by Southwest. When a young Korean woman says she recently left her marketing job at a tech company because she wants to feel fulfilled, one of Siegel's organisers pounces. ""So you're in transition?"" she says, cutting the woman off. The woman says she guesses so. But Siegel's volunteer cuts her short again. ""Would you be interested in a transitioning workshop?"" asks the volunteer, turning to Siegel and nodding eagerly. Siegel mostly earns money from his consulting, giving TED-style talks at conferences and teaching a class on meditation and technology at Stanford. MeetUp members fund its talks, but Siegel's team is keen to expand its offerings and revenue.

To start the meeting, Siegel asks three questions: how many people have a meditation or spiritual practice? How many work in science or tech? How many want to leave their jobs for something more meaningful? Each time, three quarters of the hands go up. Satisfied, Siegel says, ""We are creating a community of practice and purpose, spiritual innovation and insight."" Behind me, a young UX developer thumbing her iPhone says, ""Wow, insight."" Siegel wraps up, saying, 'We're eternally conflicted and in a lot of pain. If you think that's less than 50 per cent bullshit, you should join us."" Cost: $10 a month, $100 a year. When Mossbridge takes the mic, she is wearing a tie-dye hoodie with a heart in the centre. She runs through the archetypes we have of AI robots. They'll be obedient children, ""like slaves that do what we say"", she says, ""saving us from danger"". On a screen, she shows a clip of the robot in Interstellar carrying Anne Hathaway to safety. ""Or they'll be evil tyrants that overtake us and tell us what to do."" Something Mossbridge says we secretly desire. Or we can program them to help us achieve our greatest potential. Just as human beings are not the smartest operating systems, we are not the most benevolent. We may need to build an AI more compassionate than we are - to save us from ourselves, to shift consciousness toward enlightenment. In that way we don't have to just hope, afterward, the machine we built is nice. During the Q&A, a man stands and asks, ""What's the purpose?"" Mossbridge says, ""I imagine a world where each person is paired with a loving AI that teaches us to love better."" After the meeting, an older couple (with old-school grocery industry money) give Siegel a cheque for $200,000 to fund his MeetUps and explore AI and consciousness. The Burning Man guy, whose camp offers energy work and eye-contact jams, talks blockchains with the young UX developers. Siegel, smiling beatifically, nods his way through the crowd. Unlike the scene at The Battery, with its Adobe marketing execs and Facebook product launchers, this one seems full of people who don't want what Silicon Valley is offering, but believe technology will free them from anxieties that technology has created.

There's talk of online AIs offering bespoke psychotherapy, tailoring modalities for each of our ills, with none of the baggage that human therapists carry (even though those AIs will need to be programmed by actual humans). There's chatter about brain stimulators that will one day tickle our neuron networks so they dump on happiness chemicals like dopamine, serotonin, endorphins and oxytocin.

Advances in virtual reality might teach us, and some studies suggest they can, to become more altruistic. But, like a knife, it could cut the other way. ""You could just say, 'Who needs a planet?'"" Whiton posits to me one day, ""'I got this many pixels.'"" The brain itself might prove the ultimate battleground. Studies show that our grey matter might contain its own dimethyltryptamine, a powerful hallucinogen that can induce a mystical high. Like all hallucinogens, it has the potential to radically shift your experiences and transform your personality in a single trip. What happens when we create the enlightenment button, using tech to trigger a cascade of hallucinogens? Does the government regulate that like an illegal drug?

That's the sort of big question - how much consciousness shifting can we take? - that Whiton and Siegel can debate endlessly, like turned-on college grads. How much will others allow, before - like previous mind-expanding experiments - it all goes to hell and someone with top-down control shuts it down?

""You've got all this mindfulness stuff happening in big companies, but how much mindfulness are they really ready for?"" Whiton pondered earlier at Café Bliss. ""Do they just want their workers at their desks stress free? Because what Mike and I have found is if you meditate too long, you decide your job is not for you. I'm not sure companies want you to be self-actualised to the point where you realise, 'This isn't working in a really kind of soulless way. I got it. I'm out.'""

""It's a very real possibility,"" Siegel replied, sitting crossed-legged and leaning over his tea. ""Like, what's the dosage, right? Where people are just like, 'Well, fuck this.'""

Kevin Gray wrote about Kobalt Music Group in issue 05.15"
neuroscience;/article/mental-health-wearable-medtech;Wearables could help diagnose disorders in children earlier;"Professional athletes already use wearables. But a new study demonstrates a novel application in psychiatry, with potentially life-changing effects Throughout your life, your brain will become more rigid. In fact, if you want to create lasting behavioural change, it’s best to intervene as early as possible while your brain is still malleable. For childhood psychiatrists, early intervention is a focus of their work – which doesn’t easily fit into when internalising disorders (such as anxiety and depression) are identified.

Scientific literature has found that almost 20 per cent of children will have the experience of an internalising disorder. For children who are showing symptoms of anxiety or depression, they might not have the right words to articulate how they feel – and they might not even be aware that it’s something they could talk about to other people.

Now, a group of researchers from the University of Michigan and the University of Vermont are pioneering a new method in diagnosing children earlier – using wearables. In a study published in PLOS ONE, researchers used commercially available wearable sensors, along with a specific task, to speed up diagnosis. For children, referral between pediatricians, psychiatrists and even from parents can add significant hurdles to accessing help. As a result, the researchers involved wanted to come up with a way to analyse data more quickly, and reduce the amount of steps necessary for a diagnosis.

The concept of a wearable device that can track your medical information may sound more dystopian than appealing, but it’s not far-fetched. Athletes already use wearables – this is because the information that a device in contact with your body can gather is often more granular than externally observed behaviour. In medicine, wearables have been trialled in several different contexts for adults. Wearables add a valuable dimension with young people because children are often unable to communicate how they feel, or even don’t know that they can speak to an adult about it. “Parents and teachers tend to be better reporters of problems they can see, such as hyperactivity in ADHD,” says Emily McGinnis. “So it is important to find objective markers of early child anxiety and depression to give these kids a voice when they might otherwise be unintentionally overlooked.”

Observational methods in psychopathology are often designed with the intention of eliciting specific behaviour, often referred to as “mood induction” tasks. In this study, the researchers carried out a 90 second “mood induction” task on 63 children, some of whom had been previously diagnosed with internalising disorders. In this experiment, a facilitator led a child into a room, using scripted statements such as, “Let’s be quiet so it doesn’t wake up.” In this room was a terrarium covered with a blanket, which the facilitator yanked up to reveal a fake snake. After this, the child was reassured that the snake was fake and encouraged to play with it. Children had been previously diagnosed using a common gold-standard assessment known as the K-SADS-PL, along with the results of a questionnaire answered by their caregivers.

Typically, researchers will use a coding technique on a video of an individual participating in such a task – this often means assigning numerical values to certain kinds of movements, and then tallying them up. Those scores are then associated with certain disorders or diagnoses.

But there is so much data (and the training to process it is extensive) that even diagnosing one person takes a lot of time. What the wearable sensors were able to do is generate kinematical measures. “A kinematical measure is a quantitative measurement of how the child is moving during the task,” says Ryan McGinnis, a professor of biomedical engineering at the University of Vermont. “The wearable sensor provides a direct measurement of the acceleration and angular velocity of the child’s torso.” This generated thousands of data points which had to be processed. In order to do so, the team of researchers developed algorithms which converted that data into quantities that demonstrated the speed and angle at which the child was turning, and what that could show.

Data is used to train a binary classification model. This model then predicts whether the participant will have an internalising disorder, which is repeated for each subject. By breaking down the 90 second task into phases, researchers were able to identify differences in behaviour at specific points across their participants. What they found was the the earliest phase (“potential threat”) which was the period from 23 to 3 seconds before the child was startled, offered a good indicator of how children with those internalising disorders behaved. Children with internalising disorders tended to turn away from ambiguous threat at a greater degree than other children. In total, out of the 10 biomechanical measures that were generated by the wearable sensors, Figure 3 in the paper demonstrate that there are ranges of movement and behaviour which differ for children with diagnoses of disorders, than those without. While researchers are unlikely to be able to pinpoint a specific reason for this, they referenced previous literature which said that it may be due to attention avoidance, which is a common tactic of children with trauma and PTSD, or emotional dysregulation (having attended to the threat previously).

“The study presents intriguing, initial evidence that it might be possible to make accurate predictions regarding risk for internalising psychopathology in children,” says Andrea Danese, who is a professor of child and adolescent psychiatry at King’s College London, who was not involved in the study. “However, larger studies in representative samples are needed to test this approach, and it’s unclear whether the same algorithm could be used to make accurate predictions in a different group of children.”

But for researchers, this is not necessarily that surprising - while the use of biometric data to elicit a stronger case for a diagnosis is useful, this is something which would have become clear through the use of video. Yet, using machine learning techniques as laid out here, it became possible to speed up the process up exponentially - using 20 seconds of data, as opposed to the hours it would take otherwise. “It would have been possible to identify some of the quantities by eye through behavioural coding,” adds Ryan McGinnis. “However, many of the metrics extracted would be too subtle to be picked up by eye alone.”

There just isn’t enough data to do this on a larger scale at the moment – traditional methods takes factors such as environment into question, which would not have been possible with the use of these wearables. There are also issues around privacy and the ethics of collecting data on young children which would have to be ironed out if a commercially available technology was to be rolled out. But this study opens the field for a whole new way of using wearables, and for young people with disorders, the positive effects could last far beyond just their childhood."
neuroscience;/article/virtual-reality-dementia-technology;For people with dementia, virtual reality can be life-changing;"Virtual reality, smart clothes and reminiscence therapy are offering respite to patients and carers My grandmother is a silent statistic. One of millions of people who die each year from the neurodegenerative disease, Alzheimer’s, for which a cure is not possible.

The condition, one of a number of forms of dementia, is caused by rogue proteins that lodge and tangle in the neural networks of the brain, causing irreparable damage to the billions of neurons which transmit the electrical signals that build memories. These cells gradually die, causing memory loss and personality change, eventually halting the brain’s basic functions.

Despite decades of medical research into treatments to slow the disease’s progressive course or prevent it entirely – the field from which Pfizer notably withdrew in January, after years of setbacks – it is not yet known what causes these proteins to gather, and therefore how to remove or block them. And despite Alzheimer’s being the world’s fifth biggest killer, funding levels for research have lagged shockingly behind those for both cancer and the next biggest area of medical research, cardiovascular disease.

In the meantime, the greatest cost is in providing care and therapy for those suffering from the disease – a global total currently estimated at $818 billion; the equivalent to over one per cent of global GDP. If no effective treatment and preventive solution is found this sum will only increase, as ten million new cases of dementia are diagnosed each year. Through caring for my grandmother for the last two years of her life, I experienced the toll of dementia firsthand. A brilliant art history academic, with an enviably lightning-quick and encyclopaedic knowledge recall, it wasn’t age that caused her pin-sharp brain to finally stumble and lose its love of life. The disease took hold gradually, with just small signals of confusion and memory loss to begin with – such as getting lost on her 50m walk home from the bus stop. But it became more brutal as time went on – she no longer knew where she was and couldn’t remember that grandpa had died, and therefore had to suffer this news as if for the first time, every day. She could still have moments of absolute clarity, but gradually those moments became fewer, replaced with frustration, panic and fear, as her world closed in.

However, what did bring her respite from the battle being waged in her brain – for a few wonderful minutes at a time – were her increasingly frequent references to fragments of childhood memories. Recalling this makes me appreciate the potential of companies such as the young UK healthcare start-up, Virtue, which is applying the latest immersive technologies to the process of ‘reminiscence therapy’. While the traditional approach draws on physical visual stimulus such as photo books, or even involves substantial investment in constructing full-scale sets that recreate nostalgic scenes, Virtue has developed a new type of memory portal using virtual reality.

“It’s only now that the phone in your pocket is advanced enough and VR headsets are reducing in price that we can really democratise access to this type of impactful therapy,” Virtue's co-founder and CTO Scott Gorman says.

Virtue’s app, LookBack VR, offers a wide variety of 360 VR content and filmic experiences which chime with the memories of the target age group of the patient - arranged by destination, theme, activity or decade. Viewers can choose from experiences ranging from spending time on Brighton beach in the 1970s, to finding themselves in a 1950s tearoom, and can create a personalised playlist or ‘itinerary for time travel’ with the help of their family or carer. Their companion can see their VR headset view on a companion app via tablet, along with a series of suggested questions to help stimulate relevant conversation about that era.

“Our vision is for LookBack VR to become a global platform that can help people with dementia anywhere,” co-founder and CEO Arfa Rehman shares. “We are starting to seek partnerships with organisations and individuals to gather content from around the world.”

The proven concept of reminiscence therapy is perhaps most strikingly seen in the Dementia Villages being developed across Europe – the most well established being Hogeweyk in the Netherlands. The first UK project of its kind is slated to open in Kent, in the South East, as soon as 2020. These complete care villages are designed as enclosed havens that provide familiar surroundings and stimulation attuned to the memories and reference points of its inhabitants. They can choose to live in a 1950s-styled house, or perhaps visit the 1970s corner shop. They can even walk to the bus stop in the street if, as in the case of my grandmother, they have a sudden irrational panic that they are going to miss the bus home. Obviously a bus never comes, but the exercise is proven to be cathartic and I for one can see how that would be the case, having failed many times to convince my distraught grandmother that she was already home.

These kind of large-scale built solutions obviously take huge investment, and can only help a very select few people. So if this kind of relief can be provided for many more people, affordably, through the democratising halo of immersive technologies then I am hopeful for a world where – until a preventative treatment is found – facing dementia will not be such a truly terrible and isolating experience.

While there are many ‘smart home’ offerings being developed that aim to support the care experience; from food clocks to stimulate people with dementia to remember to eat, when they lose the instinct to feel hungry, to personal assistants such as DRESS, a new automated system just unveiled by New York University which promises to help people retain their independence and dress themselves in the morning, with remote help from a carer. But it’s the technologies that can offer respite form the emotional effects of dementia that I find most compelling, to support both people suffering with dementia and their loved ones."
neuroscience;/article/brain-distraction-procrastination-science;Here's scientific proof your brain was designed to be distracted;"Research shows that our attention can never be consistently focused. So you might as well stop trying Laser focus leads to success, or so they say. Except it actually doesn’t. Researchers have found that rather than being laser-like, attention is actually more akin to a spotlight that continually dims and comes back on again.

The research, conducted on humans and macaque monkeys, concludes that our ability to focus is designed to work in bursts of attention, rather than uninterruptedly. For instance, while it may seem that you are continuously focusing on reading this article, the reality is that you’re zooming in and out of attention up to four times per second.

The findings, the result of work carried out by scientists from Princeton University and the University of California, Berkeley are published in the journal Neuron. The researchers found that in between those bursts of attention, we are actually distracted. During those periods of distraction, the brain pauses and scans the environment to see if there is something outside the primary focus of attention that might be more important. If there is not, it re-focus back to what you were doing.

“The brain can’t process everything in the environment,” explains Ian Fiebelkorn, an associate research scholar at the Princeton Neuroscience Institute (PNI) and one of the authors of the paper. “It’s developed those filtering processes that allow it to focus on some information at the expense of other information.” The reason why we are not aware of those gaps is because the brain tricks us into perceiving reality as a continuous movie. “This is a remarkable finding that links back to the old debate of whether perception is continuous,” Sabine Kastner, a professor of psychology at PNI and leader of the project, says. “The way we perceive our sensory environment seems to be. But our findings show that this is subjective. What really happens is our perceptions go through rhythmical changes.”

The existence of those brain rhythms was discovered over 100 years ago by Prussian psychiatrist Hans Berger who, during his time in the army, received a letter from his sister accurately predicting from one of her dreams that he would fall off his horse and break his leg.

Convinced that it was a case of telepathy, Berger rushed to his lab and started experimenting. He invented the first electroencephalogram, or EEG, as a method of recording the brain’s electric activity directly from the scalp. The first EEG displayed the rhythmic pattern of brain waves. While scientists knew that they reflected the electrical pulses occurring inside our brains, up until now it remained unclear what was their cause.

The PNI studies show, for the first time, that these rhythms correspond to the alternation between two different brain states: one that is associated with focus and one that is associated with distractibility. At the peak of a brain wave, our perceptual sensitivity is at its lowest, while the trough of the wave corresponds to our highest ability to focus – the moment when we are most capable of processing the information in our environment and reacting to it.

“Our findings are ground breaking in that they show convincing evidence that brain rhythms can be linked very closely to behavioural outcome,” Kastner says. So why does our brain make us go through attention pulses at such a fast rate? The researchers suggest that it corresponds to an evolutionary advantage. “Think about when life was more dangerous,"" Fiebelkorn says. ""You would have to constantly be on the lookout, you would want to always be aware if there was something around you with bigger teeth.”

In modern life, this particular feature of the brain allows us to realise, for instance, that a car is coming as we are crossing the street. Our spotlight of attention, in this sense, has been and still is key to our survival.

This trait is not exclusive to humans. Studies on macaque monkeys showed almost identical findings, suggesting that this rhythmic form of attention is potentially common to many other species.

For Sabine Kastner, the paper’s results might also present a way to help people affected with attention deficit hyperactivity disorder (ADHD). While still being at the stage of conjecture, Kastner hypothesises that there might be a link between brain rhythms and attention-related disorders.

“Whether you are talking about the hyper-focused type of ADHD or the distractible type, you can easily draw a parallel with the two states that normal brains alternate between,” she says. “It could be that brains affected by ADHD are unable to balance between the two attentional spaces and instead get locked into one or the other.”

And while it remains unknown exactly how ADHD medication works, it is also possible that it has an effect on balancing brain rhythms. This opens a lot of opportunity, Kastner continues, to potentially retrain those brains with very simple techniques of rhythm therapy.

Ian Fiebelkorn, for his part, sees how businesses may make the most of the research. “It is possible to organise brain waves to predict when someone is more distractible,"" he says. In that sense, marketers could take advantage of our attention strobes to distract us into looking at their ad or their web page.”

In this era of fake news and endless memes, this might sound unnerving. But it might also give you a decent excuse for being distracted from your work by our latest film recommendations on Netflix. Think of it as the inevitable side effects of evolutionary advantage."
